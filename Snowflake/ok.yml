kind: Pod                              
apiVersion: v1                     
metadata:                           
  name: testpod                  
spec:                                    
  containers:                      
name: c00                     
      image: ubuntu              
      command: 
  restartPolicy: Never         # Defaults to Always


--MULTICONTAINER--

apiVersion: v1
kind: Pod
metadata:
  name: anmol
  annotations:
    description: "This is the updated one"
spec:
  containers:
  - name: anmol
    image: busybox
    command: ["sh", "-c", "echo 'hello this is anmol' && sleep 5"]
  - name: abhishek
    image: busybox
    command: ["sh", "-c", "echo 'hello this is abhishek' && sleep 5"]
  restartPolicy: Never


-- kubectl logs anmolabhi -f abhishek
-- kubectl exec env -c anmol -it -- sh

--ENVIRONAMENT VARIABLES--

apiVersion: v1
kind: Pod
metadata:
  name: env
spec:
  containers:
  - name: anmol
    image: busybox
    command: ["sh", "-c", "echo 'This is env' && sleep 360"]
    env:
    - name: mynameC
      value: Anmol

    for checking:-  echo $myname

    -- PORTS containers:--

apiVersion: v1
kind: Pod
metadata:
  name: testpod4
spec:
  containers:
    - name: c00
      image: httpd
      ports:
        - containerPort: 80

--- MULTI PODS--

apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
    - name: pod1
      image: httpd
      ports:
        - containerPort: 80

---
apiVersion: v1
kind: Pod
metadata:
  name: pod2
spec:
  containers:
    - name: pod2
      image: busybox
      command: ["sh", "-c", "echo 'Hi this is multipod' && sleep 500"]

   
------ LABELS-----
    
    kubectl label pod anmol class=higher     (label banana ouside manifest yml)
    kubectl get pods --show-labels           (current labels ki info )
    kubectl get pods -l env=production       (give the pod which has that label)  

------ Labels & Selectors------

Equality based (=, !=)
Set based (in, notin, exists)  [Multiple values]

get pods -l 'name in (a1,a2)'
get pods -l 'name notin (a1,a2)'


apiVersion: v1
kind: Pod
metadata:
  name: anmol
  labels:
    name: a1
    env: production
    company: slk
spec:
  containers:
    - name: myapp
      image: busybox
      command: ["sh", "-c", "echo 'Hi this is my label' && sleep 500"]


------- NODE SELECTORS-------
 
1 master has 4 nodes (if we want to create on specific node then we use node selector)
e.g   in labels (hardware=t2.medium  )  [So that node have t2.medium instance vha pod create honge]

    command:  kubectl label nodes (private ip of node) hardware=t2.micro
              kubectl describe pod anmol

--IF want to add in code--- (It comes under spec)

apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: mycontainer
      image: busybox
  nodeSelector:
    disktype: ssd


------ Scaling & Replications ------

1. Reliability  (Agar ek pod kaharab hoga , to doosre par shift ho jaayega[]replica)
2. Scaling (agar load bdh rha hai (suppose 3 pod hai) , to load bdthe time POD auto create ho jaayege node pe)
3. Load Balancers (Multiple PODS ko balance krna , equally)
4. Rollimng Update

Suppose 1 Node{500 POD hai or utne hi honge } agar need jyda hai to automnatically (new nodes bengi or unpar baaki 
podes create honge)

5. ReplicationController =
      Suppose, We decsribe our desired state =2 (agar ek fail hogye , to ek or create honge)
      **agar ek delete hogya , fir bhi 2 hi honge ** (bcz we put desired state =2)

apiVersion: v1
kind: ReplicationController
metadata:
  name: myreplica                 [Object name]
spec: 
  replicas: 2                    [Ye hamara desired state hai, means itne honge hi , if one got deleted]  [POD creation]
  selector:
    hardware: t2.medium               [ POD vhi create honge jo node t2. medium par hoga]
  template:                          {POD kaisa bngea ye batayega}
    metadata:
      name: mypod
      labels:
        myname: Anmol                 [Lables]
        env: production
    spec:
      containers:
      - name: c1
        image: busybox
        command: ["/bin/bash", "-c", "while true; do echo Hello; sleep 5; done"]

kubectl describe rc myreplica

kubectl scale --replicas=4 rc -l myname=Anmol   (jha par label mynam=anmol hai us pod ko 4 baar  banaodo)
                                                 (2 phele bnaaye the ab total 4 hogye)
kubectl scale --replicas=1 rc -l myname=Anmol    (DOWN krna)

***Replica_Set** (equlaity based as well as set based dono par kaam krta hai)
means (agar scaling krni ho to apn labels par multiple condition daal skte hai 

like :- kubectl scale rs myrs --replicas=1

Selector = ‚ÄúWhich Pods should I manage?‚Äù
Template = ‚ÄúWhat should my Pods look like?‚Äù

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myrs
spec:
  replicas: 2
  selector:
    matchExpressions:
      - key: myname
        operator: In
        values:
          - Anmol
          - Abhi
      - key: env
        operator: NotIn
        values:
          - production
  template:
    metadata:
      name: testpod7
      labels:
        myname: Anmol
        env: staging
    spec:
      containers:
        - name: c2
          image: ubuntu
          command: ["/bin/bash", "-c", "while true; do echo Technical-Guftgu; sleep 5; done"]


***************Deployment_ROLLBACK********

Deployment provides decraltive set for Pods & Replicasets

             Deployment ---------
             |                   |
             replica set1         replicaset2
             |                        |
             POd1                     Pod 2

             agar Pod1 me kuch update hoga to deplpoyemnt object ek nya replica bna dega (old wale pod stop/delete
              ho jaayege)
             or agar rollback krna ho Pod2 to Pod 1 (ROLLBACK object kr dega)
    saara code Pod2 kaa pod 1 me aajyega (blhe hi dono me no. of pods alag ho)
    Scaling bhi kr skte hai 

    jo pod sbe aakhir me create honge vo sbse phele delete honge
    We can pause the depolyment(after fixing we can resume)

kubectl set image deployment/mydeployments c4=busybox:latest (image ko update krne ke liye)

**kubectl get deploy   (jab ko apna deployment cluster me dekhna/inspect krna ho)
                      (Name, age, ready, up to date .... ye sab information dega aapko aapke deploy kaa)

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydeployments
spec:
  replicas: 2
  selector:
    matchLabels:
      name: deployment
  template:
    metadata:
      labels:
        name: deployment
    spec:
      containers:
        - name: c4
          image: busybox
          command: ["/bin/bash", "-c", "while true; do echo Hello; sleep 5; done"]

kubectl describe deploy mydeployments (kitne rs hai pods hai)
kubectl get rs (kitne replica sets hai)


**Rollout_Commands:-
kubectl rollout status deployment mydeployment
kubectl rollout history deployment mydeployment {Kitni baar changes kiye hai, vo sab aajyega}
kubectl rollout undo (previous version par aajeyga replica)
kubectl rollout undo deployment/<name> --to-revision=<n> (specifix version)
-------------------------------------------------------------------------------------------------
**ReplicaSet ‚Äì Complete Bullet Points
üîπ What is a ReplicaSet?

Pods ko desired count me maintain karta hai (e.g., always 3 Pods running).
Agar koi Pod crash ho jaye ‚Üí ReplicaSet naya Pod bana deta hai.
ReplicaSet = Pod lifecycle manager.

üîπ Why we don‚Äôt use ReplicaSet directly?

Direct ReplicaSet me:
‚ùå Rolling updates nahi
‚ùå Rollback nahi
‚ùå Version history nahi
Deployment hi in sab ko automatically manage karta hai.

üîπ How ReplicaSet is used today?

**Mostly Deployment ke through indirectly use hota hai.
**Deployment har update par new ReplicaSet create karta hai.


‚≠ê How Deployment Uses ReplicaSet Internally (Bullet Points)
üîπ 1. Deployment created ‚Üí first ReplicaSet created

Aap YAML apply karte ho ‚Üí Deployment ‚Üí ReplicaSet v1 banata hai ‚Üí Pods v1 run hote hain.

üîπ 2. You update Deployment ‚Üí new ReplicaSet

Image update, env update, config change = ReplicaSet v2 create hota hai.
Pods v2 rollout hote hain.

üîπ 3. Rolling update = two ReplicaSets

Old ReplicaSet (v1) ‚Üí scale down
New ReplicaSet (v2) ‚Üí scale up
Zero downtime process.

üîπ 4. Deployment never touches Pods directly

Deployment ‚Üí ReplicaSet ‚Üí Pod
(Indirect control chain)


‚≠ê Rollout ‚Äì Easy Bullet Points (Hinglish)
üîπ What is rollout?

‚ÄúRollout‚Äù means deployment ka new version apply karna ‚Äî new pods create hona.

üîπ What triggers a rollout?

Image change
Env var change
Label change
Config update
Pod template me koi bhi change

üîπ Rollout process

Deployment creates new ReplicaSet
Slowly new pods start
Old pods stop/delete
No downtime if strategy = RollingUpdate

üîπ Commands

Check rollout status
kubectl rollout status deployment <name>

See rollout history
kubectl rollout history deployment <name>


‚≠ê Rollback ‚Äì Easy Bullet Points (Hinglish)
üîπ What is rollback?

Rollback = deployment ko previous stable version par wapas lana.
Deployment old ReplicaSet ko activate kar deta hai.

üîπ When rollback works?

Jab old ReplicaSet exist karta ho
Deployment ke paas atleast 1 revision stored ho
Pod template previously change hua ho

üîπ When rollback does NOT work?

No old ReplicaSet created (first deployment)
You never updated pod template
old ReplicaSet manually delete kar diya
revisionHistoryLimit: 0 set ho

üîπ Rollback command
kubectl rollout undo deployment <name>Show more lines
üîπ What happens internally?

Old ReplicaSet scale-up
New ReplicaSet scale-down
Old version ke pods wapas create ho jate hain


‚≠ê Ultra-Simple Summary

ReplicaSet = Pods ko maintain karne wala worker
Deployment = Boss ‚Üí ReplicaSet create/update/delete karta hai
Rollout = New version deploy
Rollback = Old version par wapas jana
Deployment ke bina ReplicaSet = No rolling update, no rollback
Isliye hum Deployment hi use karte hain, ReplicaSet manually nahi.

-------------------------------------------------------------------------------------------------
-------NETWORKING-----

apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
    - name: c00
      image: busybox
      command: ["/bin/bash", "-c", "while true; do echo Anmol; sleep 5; done"]
    - name: c01
      image: httpd
      ports:
        - containerPort: 80

Testing (Con1 to cont2  inside same POD same NODE)
[root@ip-172-31-89-65 ec2-user]# kubectl exec pod1 -it -c c00 -- /bin/sh
/ # curl localhost:80
/ # wget -qO- http://localhost:8

Testing (Pod to Pod inside same node)

[root@ip-172-31-89-65 ec2-user]# kubectl get pods -o wide
NAME              READY   STATUS    RESTARTS   AGE     IP            NODE                       NOMINATED NODE   READINESS GATES
myreplica-h99h6   1/1     Running   0          40m     10.244.0.8    my-cluster-control-plane   <none>           <none>
pod4              1/1     Running   0          6m47s   10.244.0.11   my-cluster-control-plane   <none>           <none>
pod5              1/1     Running   0          19m     10.244.0.10   my-cluster-control-plane   <none>           <none>

# use ip address of different pods to communicate b/w pods
# curl ipaddress 

-------SERVICES---------

# tumhara pod ip1 par chl rha tha , par kuch issue hua pod kharab hogya or replica set ki need se 1 or pod bn gya
# uss case me POD ki Ip change hoti hai we know...

yha par hum SERVICEs object ko replica set ke upar lgaate hai (Virtual IP provide krta hai) (MAP krta hai replica ko POD ko)
# so agar POD ki Ip change hoti hai to bhi Hmm usko SERVICES ki IP se access kr skte hai, respose same hoga
# replica ke alawa bhi or bhi object par lgaa skte hai , e.g deployment bla bla (static IP dega SERVICES )
# Inshort (SERVICES ek bridge hai PODS and end user ke beech , jo virtual ip deta hai)
# Mapping kaa kaam kube proxy krta hai (virtual ip map to Pod Ip)
# Cluster Ip (default) , Elastic Ip and Load balancers  (Ye treeke hote hai services ke)
## 2 nodes in one cluster each node has one pod (If they want to communicate with other then thye use Cluster Ip)
# Cluster Ip ke upar Node Port lgta hai or fir uske upar Load balancer lgta hai
# cluster ke bahar IP use nhi hoti

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydeployments
spec:
  replicas: 1  
  selector:                                      # tells the controller which pods to watch/belong to
    matchLabels:
      name: deployment
  template:
    metadata:
      name: testpod1
      labels:
        name: deployment
    spec:
      containers:
        - name: c00
          image: httpd
          ports:
            - containerPort: 80

====================
apiVersion: v1
kind: Service
metadata:
  name: demoservice
spec:
  selector:
    name: deployment  # Targets pods with this label
  ports:
    - port: 80         # Port exposed by the service
      targetPort: 80   # Port on the pod
  type: ClusterIP      # Default type, internal access only
                     

[root@ip-172-31-89-65 ec2-user]# kubectl get svc
NAME                  TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
demoservice           ClusterIP      10.96.112.212   <none>        80/TCP         61s

----NODEPORT-----
## U can access your pod outside cluster by public dns with port 
# We dont need ip on this , by port we can access (Port map krna hota hai virtual IP ko(cluster ip ko))

apiVersion: v1
kind: Service
metadata:
  name: demoservice
spec:
  selector:
    name: deployment  # Targets pods with this label
  ports:
    - port: 80         # Port exposed by the service (client uses this)
      targetPort: 80   # Port on the pod (container port (app listens here))
  type: NodePort

  service/demoservice configured
[root@ip-172-31-89-65 ec2-user]# kubectl get svc
NAME                  TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
demoservice           NodePort       10.96.112.212   <none>        80:32424/TCP   45h

We got port above for nortport
kubectl describe svc demoservice (get the details about Nodeport)

------------------------------------------------------------------------
‚≠ê 1. ClusterIP (Internal‚Äëonly Service)
üîπ Simple meaning:
Service sirf cluster ke andar accessible hota hai ‚Äî bahar se koi access nahi.
üîπ You use it when:

Microservices ek‚Äëdusre se baat kar rahe hote hain
Example: backend ‚Üí database, frontend ‚Üí backend (within cluster only)

üîπ Key points:

Default service type in Kubernetes
Best for internal communication
Fast and secure because it never exposes anything to outside world


‚≠ê 2. NodePort (Basic External Access)
üîπ Simple meaning:
Cluster ke har node par ek public port open hota hai
(30000‚Äì32767 range), jisse service access hoti hai.
üîπ You use it when:

You want simple external access
No cloud load balancer available
Dev/test environment me quick access chahiye

üîπ Key points:

Access format: NodeIP : NodePort
Exposes service to outside world
Internally still uses ClusterIP (NodePort ‚Üí ClusterIP ‚Üí Pod)
Not ideal for production load (limited features)


‚≠ê 3. LoadBalancer (Production‚Äëgrade External Access)
üîπ Simple meaning:
Cloud provider (AWS, GCP, Azure) ek public Load Balancer create karta hai
jo traffic ko pods tak distribute karta hai.
üîπ You use it when:

Production application expose karna ho
Need: public IP + traffic balancing + reliability
Multiple pods handle heavy traffic

üîπ Key points:

Internet ‚Üí LoadBalancer ‚Üí NodePort ‚Üí ClusterIP ‚Üí Pods
Automatically creates a public IP
Best for large-scale and real-world applications
Requires cloud infrastructure (not for bare-metal without extra setup)


üîó Relationship

ClusterIP is the base.
NodePort builds on ClusterIP for external access.
LoadBalancer builds on NodePort for cloud-based load balancing.

------------------------------------------------------------------------
-------VOLUME------

# volukme POD par connected hota hai
# agar container delete hota hai, to volume par farak nhi pdta
# new container par same volume aajayega
# only inside POD volume accessible hota hai

# Ek POD me volume ko multiple containers ek saath attach kr skte hai

----- VOLUME TYPES---
nfs , elastic block storage, gitrepo

--- Empty DIR---

# agar POD delete/crash hogya to
# new POD bnane par , empty volume create hogi

# Hum mount krge volume ko different containers par, jo bhi update hoga sbka sab volume par data jaayega

apiVersion: v1
kind: Pod
metadata:
  name: myvolemptydir
spec:
  containers:
    - name: c1
      image: busybox
      command: ["/bin/sh", "-c", "sleep 15000"]
      volumeMounts:
        - name: xchange
          mountPath: "/tmp/xchange"
    - name: c2
      image: busybox
      command: ["/bin/sh", "-c", "sleep 10000"]
      volumeMounts:
        - name: xchange
          mountPath: "/tmp/data"
  volumes:
    - name: xchange
      emptyDir: {}

## We can access same data via two containers

[root@ip-172-31-89-65 ec2-user]# kubectl exec myvolemptydir -c c1 -it -- /bin/sh
/ # ls
bin           etc           lib           proc          product_uuid  sys           usr
dev           home          lib64         product_name  root          tmp           var
/ # cd tmp
/tmp # ls
xchange
/tmp # cd xchange
/tmp/xchange # ls -ltr
total 0
/tmp/xchange # touch abc.txt
/tmp/xchange # cd 
~ # exit -------------------------------------------------------------------------------------------------
[root@ip-172-31-89-65 ec2-user]# kubectl exec myvolemptydir -c c2 -it -- /bin/sh
/ # ls
bin           etc           lib           proc          product_uuid  sys           usr
dev           home          lib64         product_name  root          tmp           var
/ # cd tmp
/tmp # 'ls

/tmp # ls
data
/tmp # cd data
/tmp/data # ls -ltr
total 0
-rw-r--r--    1 root     root             0 Jul 23 23:33 abc.txt


------- HOSTPATH----

# Ek POD ko doosri POD ke volume ko access krna
# humko host path me mount krna hota hai

apiVersion: v1
kind: Pod
metadata:
  name: myvolhostpath
spec:
  containers:
    - name: testc
      image: busybox
      command: ["/bin/sh", "-c", "sleep 15000"]
      volumeMounts:
        - name: testvolume
          mountPath: /tmp/hostpath
  volumes:
    - name: testvolume
      hostPath:
        path: /tmp/data
        type: DirectoryOrCreate

        -------------------------------------------------------------------------------------------------

----- Persistent Volume ----

# Cluster wide resource
# Multiple woker nodes PV ko access kr skte hai (type of NAS)
# POD ke delete hone par kuch farak nhi pdega
# We create EBS  and we connect to node (Its always available)

## **EBS ko divide krge volumes me or nodes ko dege**
# Suppose 100 gb hai EBS , and jitni volume chaaiye vo hum pod par define krege (like 5gb or 6gb)
# PV objects dene hote hai , usme se distribute hote hai
# Shared volume accross cluster 
# Claim krna pdta hai , kitni volume chaaiye
# Hum release bhi kr skte hai

# phele volume define krge , then claim krege usme se kitni chaaiye jo define kri hai

----VOLU DEFINE--

apiVersion: v1
kind: PersistentVolume
metadata:
  name: myebsvol
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  awsElasticBlockStore:
    volumeID: vol-042dda85865440f2c # YAHAN APNI EBS VOLUME ID DAALO
    fsType: ext4

------CLAIM KRNA---

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myebsvolclaim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

[root@ip-172-31-89-65 ec2-user]# kubectl apply -f defvol.yml
persistentvolume/myebsvol created
[root@ip-172-31-89-65 ec2-user]# kubectl get pv
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
myebsvol   1Gi        RWO            Recycle          Available                          <unset>     
                     12s
[root@ip-172-31-89-65 ec2-user]# vi claimvol.yml
[root@ip-172-31-89-65 ec2-user]# kubectl apply -f claimvol.yml
persistentvolumeclaim/myebsvolclaim created
[root@ip-172-31-89-65 ec2-user]# kubectl get pvc
NAME            STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
myebsvolclaim   Pending         

----Attached Into PODS----

apiVersion: apps/v1
kind: Deployment
metadata:
  name: pvdeploy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mypv
  template:
    metadata:
      labels:
        app: mypv
    spec:
      containers:
        - name: shell
          image: busybox
          command: ["bin/sh", "-c", "sleep 10000"]
          volumeMounts:
            - name: mypd
              mountPath: "/tmp/persistent"
      volumes:
        - name: mypd
          persistentVolumeClaim:
            claimName: myebsvolclaim

-------LivenessProbe----HEALTHCHECK------

# Container ki health check hoti hai(Ek time ke baad)
# if response 0 aayega means good  hai
# Otherwise POD ko regenerate kraayega (kubelet krega)
# Conatiner alive hai ke nhi, uske under application running hai ke nhi
--Parameters---
# Initial Delay dete hai (Like 5 sec , means after 5 sec test should be start)
# period second (eg 20 sec , means fater every 30 sec health check hoga)
# timeout second (e.g. agar 30 sec tak kuch bhi nhi ayaa to fail ho jaayega)

Liveness Probe

Purpose: Checks if the container is still alive (running).
If the liveness probe fails:

Kubernetes kills the container and restarts it (based on restart policy).
Use Case: Detect deadlocks or stuck processes.
------------------------------

Readiness Probe

Purpose: Checks if the container is ready to serve traffic.
If the readiness probe fails:

Kubernetes removes the pod from Service endpoints (no traffic sent).
Use Case: Wait until app finishes initialization or dependencies are ready.
------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: mylivenessprobe
  labels:
    test: liveness
spec:
  containers:
    - name: liveness
      image: busybox
      args:
        - /bin/sh
        - -c
        - touch /tmp/healthy; sleep 1000
      livenessProbe:
        exec:
          command:
            - cat
            - /tmp/healthy
        initialDelaySeconds: 5
        periodSeconds: 5
        timeoutSeconds: 30


# Path par jakar "echo $" dena hoga (agar 0 yaa means healthy hai, otherwise unhealthy)

-----------------CONFIG & SECERTS-----------------

# Virtual memory create krta hai or vha .conf file save krta hai
# agar koi POD kaharab hoga , to nye wale ke saath automatic aajeygi (jo path doge aap us par)
# configurtion maintain krt hai (plain text file hoti hai)
# e.g (Testing , QA or production  jab file jaati hai, comit hoke)
# jaise jaise move kregi (vaise vaise map hogi files)

# secerts (contains id, pass), sensitive data
# ***readable nhi hota
# encrypted hota hai  (txt or yaml file)

# Humko virtual memory ko map krna hota hai kisi path par


# Config or secerts 2 treh se access hota hai (volume in POD and Environment variable)

---CRASH LOOP BACK FIX--

To fix a CrashLoopBackOff issue in Kubernetes, i'll start by 
checking the pod logs using kubectl logs <pod-name> --previous to 
identify the root cause of the crash. If the problem is due to 
application errors, correct the code or configuration and redeploy. 
For missing environment variables or secrets, I'll ensure that they are properly 
defined in ConfigMaps or Secrets and referenced in the pod specifications. 
If resource limits are too strict and causing OOMKilled, I'll increase CPU 
or memory requests and limits in the deployment YAML. Also, I'll verify 
that liveness and readiness probes are correctly configured, 
because incorrect probes can cause unnecessary restarts. After making changes, 
apply the updated manifest using kubectl apply -f <file> and monitor the 
pod with kubectl get pods --watch to confirm it moves to a Running state. 

mera first step hamesha pod ke logs check karna hota hai using 
kubectl logs <pod-name> --previous, taaki mujhe exact pata chale container 
kis reason se crash ho raha hai. Mostly yeh issues application errors, 
missing configs. Agar logs me application‚Äëlevel bug ya incorrect configuration milti hai, 
toh main code ya config fix karke image redeploy kar deta hoon. Kabhi-kabhi CrashLoopBackOff 
missing environment variables ya Secrets ki wajah se bhi hota hai, toh main ensure 
karta hoon ki ConfigMaps/Secrets properly defined ho . Production me ek common reason 
OOMKilled bhi hota hai, jisme container memory limit cross kar deta hai. Aise case me main
 CPU/memory requests aur limits adjust kar deta hoon. Saath hi main liveness aur readiness 
probes bhi recheck karta hoon, kyunki galat probe configuration bhi container 
ko unnecessary restarts me daal deta hai. All fixes apply karne ke baad main 
kubectl apply se manifest update karta hoon aur kubectl get pods --watch se 
monitor karta hoon until pod stable Running state me aa jaye.

For OOM error:- kubectl top pod <pod-name> -n <namespace>

----- Load in PODS----

In Kubernetes, you can check the load on pods by monitoring their 
resource usage, primarily CPU and memory. The most common way is using 
the kubectl top pod command, which requires the Metrics Server to be 
installed in the cluster. This gives real-time resource consumption 
for each pod. Additionally, you can use kubectl describe pod <pod-name> 
to view resource requests and limits, though this shows configuration 
rather than live usage. For more detailed monitoring, tools like 
Prometheus with Grafana or CLI dashboards like K9s provide historical 
and real-time metrics. Kubernetes itself doesn‚Äôt measure load directly, 
so external monitoring solutions are often used for deeper insights.

In Kubernetes, hum pod ka load primarily uske CPU aur memory usage se 
measure karte hain. Quick real-time usage dekhne ke liye hum kubectl top 
pod use karte hain, jo tabhi kaam karta hai jab cluster me Metrics Server 
installed ho. Agar hume pod ki resource configuration samajhni ho‚Äîjaise 
requests aur limits‚Äîtab hum kubectl describe pod dekhte hain, but woh 
live usage nahi dikhata. Real production environments me teams usually 
Prometheus + Grafana use karti hain, jisse detailed dashboards, alerts,
 aur historical data milta hai. Overall, Kubernetes 
 directly ‚Äúload‚Äù nahi measure karta, isliye external 
monitoring tools load analysis ke liye essential hote hain.
AWS EKS me aap CloudWatch Container Insights bhi use kar sakte ho‚Äîye AWS ka native 
solution hai jo cluster, node, pod, aur container-level metrics deta hai.


------------------------------
Sonarcube---

SonarQube Jenkins ‡§Æ‡•á‡§Ç ‡§á‡§∏ ‡§§‡§∞‡§π ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§Ø‡§π ‡§Ü‡§™‡§ï‡•á ‡§ï‡•ã‡§° ‡§ï‡•ã 
bugs, vulnerabilities, ‡§î‡§∞ code smells ‡§ï‡•á ‡§≤‡§ø‡§è analyze ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ 
Jenkins pipeline ‡§Æ‡•á‡§Ç build ‡§î‡§∞ test ‡§ï‡•á ‡§¨‡§æ‡§¶, Jenkins SonarScanner 
(Maven, Gradle ‡§Ø‡§æ CLI ‡§ï‡•á ‡§ú‡§∞‡§ø‡§è) ‡§ö‡§≤‡§æ‡§§‡§æ ‡§π‡•à, ‡§ú‡•ã analysis results ‡§ï‡•ã 
SonarQube server ‡§™‡§∞ ‡§≠‡•á‡§ú‡§§‡§æ ‡§π‡•à‡•§ Server ‡§á‡§® results ‡§ï‡•ã Quality Gate 
‡§ï‡•á against evaluate ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, ‡§ú‡•ã ‡§§‡§Ø ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§ï‡•ã‡§° standards ‡§™‡§∞ 
‡§ñ‡§∞‡§æ ‡§â‡§§‡§∞‡§§‡§æ ‡§π‡•à ‡§Ø‡§æ ‡§®‡§π‡•Ä‡§Ç‡•§ ‡§Ö‡§ó‡§∞ Quality Gate fail ‡§π‡•ã ‡§ú‡§æ‡§§‡§æ ‡§π‡•à, ‡§§‡•ã Jenkins 
waitForQualityGate() step ‡§ï‡§æ ‡§á‡§∏‡•ç‡§§‡•á‡§Æ‡§æ‡§≤ ‡§ï‡§∞‡§ï‡•á pipeline ‡§ï‡•ã fail ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ 
‡§π‡•à‡•§ ‡§á‡§∏‡§∏‡•á ‡§Ø‡§π ‡§∏‡•Å‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§π‡•ã‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§ï‡•á‡§µ‡§≤ 
clean ‡§î‡§∞ secure code ‡§π‡•Ä ‡§Ü‡§™‡§ï‡•á CI/CD pipeline ‡§Æ‡•á‡§Ç ‡§Ü‡§ó‡•á ‡§¨‡§¢‡§º‡•á‡•§



Quality Gate ‡§ï‡§æ ‡§Æ‡§§‡§≤‡§¨ ‡§¨‡§π‡•Å‡§§ ‡§Ü‡§∏‡§æ‡§® ‡§π‡•à: ‡§Ø‡§π SonarQube ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§ö‡•á‡§ï‡§™‡•â‡§á‡§Ç‡§ü 
‡§π‡•à ‡§ú‡•ã ‡§§‡§Ø ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§Ü‡§™‡§ï‡§æ ‡§ï‡•ã‡§° ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à ‡§Ø‡§æ ‡§®‡§π‡•Ä‡§Ç‡•§ ‡§á‡§∏‡§Æ‡•á‡§Ç ‡§ï‡•Å‡§õ ‡§®‡§ø‡§Ø‡§Æ ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç ‡§ú‡•à‡§∏‡•á ‚Äì 
‡§ï‡•ã‡§à ‡§®‡§Ø‡§æ ‡§¨‡§°‡§º‡§æ bug ‡§®‡§π‡•Ä‡§Ç ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è, ‡§ï‡•ã‡§° ‡§ï‡§µ‡§∞‡•á‡§ú ‡§ï‡§Æ ‡§∏‡•á ‡§ï‡§Æ 80% ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è, 
‡§î‡§∞ ‡§ï‡•ã‡§à security issue ‡§®‡§π‡•Ä‡§Ç ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è‡•§ ‡§Ö‡§ó‡§∞ ‡§Ø‡•á ‡§®‡§ø‡§Ø‡§Æ ‡§™‡•Ç‡§∞‡•á ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç ‡§§‡•ã Quality 
Gate Pass ‡§π‡•ã‡§§‡§æ ‡§π‡•à, ‡§®‡§π‡•Ä‡§Ç ‡§§‡•ã Fail‡•§ Jenkins ‡§Æ‡•á‡§Ç ‡§Ö‡§ó‡§∞ Quality Gate fail ‡§π‡•ã ‡§ú‡§æ‡§è, 
‡§§‡•ã pipeline ‡§∞‡•Å‡§ï ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à ‡§§‡§æ‡§ï‡§ø ‡§ñ‡§∞‡§æ‡§¨ ‡§ï‡•ã‡§° deploy ‡§® ‡§π‡•ã‡•§


Ingress Controller ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•ã‡§§‡§æ ‡§π‡•à? (Kubernetes)
Short answer:
Ingress Controller Kubernetes cluster ‡§ï‡•á ‡§Ö‡§Ç‡§¶‡§∞ ‡§ö‡§≤‡§®‡•á ‡§µ‡§æ‡§≤‡§æ ‡§è‡§ï component ‡§π‡•à 
‡§ú‡•ã Ingress resources ‡§ï‡•ã ‡§™‡§¢‡§º‡§ï‡§∞ ‡§Ü‡§™‡§ï‡•á HTTP/HTTPS traffic ‡§ï‡•ã ‡§∏‡§π‡•Ä Service/Pod 
‡§§‡§ï route ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ‡§Ø‡§π ‡§Ö‡§ï‡•ç‡§∏‡§∞ reverse proxy / load balancer ‡§ï‡•Ä ‡§§‡§∞‡§π ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à 
‡§î‡§∞ TLS termination, path/host-based routing, ‡§î‡§∞ ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ/observability ‡§ú‡•à‡§∏‡•Ä ‡§∏‡•Å‡§µ‡§ø‡§ß‡§æ‡§è‡§Å ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§

‡§ï‡•ç‡§Ø‡•ã‡§Ç ‡§ö‡§æ‡§π‡§ø‡§è?

NodePort/LoadBalancer ‡§∏‡•á ‡§Ü‡§™ ‡§∏‡•Ä‡§ß‡•á service expose ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç, ‡§≤‡•á‡§ï‡§ø‡§®:

‡§π‡§∞ service ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ö‡§≤‡§ó external LB ‡§Æ‡§π‡§Å‡§ó‡§æ ‡§î‡§∞ ‡§ú‡§ü‡§ø‡§≤ ‡§π‡•ã ‡§ú‡§æ‡§§‡§æ ‡§π‡•à‡•§
Hostname ‡§Ø‡§æ path ‡§ï‡•á ‡§Ü‡§ß‡§æ‡§∞ ‡§™‡§∞ advanced routing ‡§Æ‡•Å‡§∂‡•ç‡§ï‡§ø‡§≤ ‡§π‡•à‡•§


Ingress + Ingress Controller ‡§ï‡•á ‡§∏‡§æ‡§•:

‡§è‡§ï ‡§π‡•Ä entry point (DNS) ‡§∏‡•á multiple apps serve ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§
foo.example.com ‡§î‡§∞ bar.example.com ‡§Ø‡§æ /api, /web ‡§ú‡•à‡§∏‡•á paths ‡§ï‡•ã ‡§Ö‡§≤‡§ó-‡§Ö‡§≤‡§ó services ‡§§‡§ï route 
‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§
TLS/Certificates centrally manage ‡§π‡•ã ‡§ú‡§æ‡§§‡•á ‡§π‡•à‡§Ç‡•§




Core Concepts

Ingress Resource: Kubernetes object ‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç rules ‡§≤‡§ø‡§ñ‡§§‡•á ‡§π‡•à‡§Ç (host, path, backend service+port).
Ingress Controller: ‡§µ‡§π software ‡§ú‡•ã ‡§á‡§® rules ‡§ï‡•ã ‡§™‡§¢‡§º‡§ï‡§∞ ‡§µ‡§æ‡§∏‡•ç‡§§‡§µ‡§ø‡§ï proxy/LB configure ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
Common Controllers:

NGINX Ingress Controller
Traefik
HAProxy
Cloud-specific: AWS ALB Ingress Controller, GKE Ingress, Azure Application Gateway Ingress, etc.




‡§Ø‡§π ‡§ï‡•à‡§∏‡•á ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à (Flow)

‡§Ü‡§™ ‡§è‡§ï Ingress YAML ‡§¨‡§®‡§æ‡§§‡•á ‡§π‡•à‡§Ç ‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç host/path routing rules ‡§π‡•à‡§Ç‡•§
Ingress Controller (cluster ‡§ï‡•á ‡§Ö‡§Ç‡§¶‡§∞ ‡§è‡§ï Deployment/DaemonSet) ‡§á‡§® rules ‡§ï‡•ã watch ‡§ï‡§∞‡§ï‡•á ‡§Ö‡§™‡§®‡•Ä 
internal config ‡§¨‡§®‡§æ‡§§‡§æ ‡§π‡•à‡•§
External traffic DNS/LB ‡§∏‡•á Controller ‡§§‡§ï ‡§Ü‡§§‡§æ ‡§π‡•à‡•§
Controller reverse proxy ‡§ï‡•Ä ‡§§‡§∞‡§π ‡§∏‡§π‡•Ä Kubernetes Service ‡§ï‡•ã request forward ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, 
‡§ú‡•ã Pods ‡§§‡§ï ‡§ú‡§æ‡§§‡§æ ‡§π‡•à‡•§
(Optional) Controller TLS terminate ‡§ï‡§∞ ‡§¶‡•á‡§§‡§æ ‡§π‡•à ‡§î‡§∞ HTTP ‚Üí Service ‡§§‡§ï plain traffic ‡§≠‡•á‡§ú‡§§‡§æ ‡§π‡•à‡•§


------------------------------------------------------------------------
Kubelet Kubernetes ka node agent hai jo pods ko start, stop, monitor karta hai aur node 
ko healthy state me maintain karne ka kaam karta hai.

Pod Pending ka matlab hota hai ki Kubernetes ne pod create toh kar diya, par uske liye 
koi suitable node nahi mil rahi scheduling ke liye

1. Check pending reason
Sabse pehle reason dekho:

kubectl describe pod <pod-name>
Yahan exact reason mil jaata hai ‚Äî jaise CPU/Memory shortage, taints, 
nodeSelector mismatch, PVC issues, etc.

‚úÖ 2. Not enough CPU/Memory? ‚Üí Node badhao
Pod ke resource requests zyada hain?
Fix:

Pod ke resource requests kam karo
Ya cluster autoscaler se naya node add hone do
Ya manually node group scale-up karo


‚úÖ 3. NodeSelector / Taints issue
Pod kisi specific node label ya taint ko match nahi kar raha hota.
Fix:

Labels theek karo
Taints ko remove/adjust karo
Pod ke tolerations add karo


‚úÖ 4. PVC / Storage issue
Agar pod ek PVC use karta hai aur wo provision nahi ho rahi:
Fix:

StorageClass verify karo
Volume size/zone match karo
EBS permission issues check karo (EKS)


‚úÖ 5. ImagePull issue
Private repo? Wrong credentials?
Fix:

imagePullSecrets add karo
Image name/tag sahi karo

------------------------------------------------------------------------

How to Fix Node NotReady ‚Äî :
Agar Kubernetes/EKS me koi node NotReady dikh raha ho, to iska simple matlab hai ki node 
healthy nahi hai ya control plane se connect nahi kar pa raha. Isko fix karne ka easy 
troubleshooting flow yeh hai

Node NotReady usually kubelet, CNI, disk, ya network issue hota hai ‚Äî main describe se 
reason check karta hoon, kubelet restart karta hoon, disk clean karta hoon, aur last 
option me node reboot/replace karta hoon.

‚úÖ 1. Node ka exact reason dekho
Sabse pehle:
Shellkubectl describe node <node-name>
Yahan se pata chalega issue kya hai ‚Äî DiskPressure, MemoryPressure, NetworkUnavailable, kubelet crash, CNI issue, etc.

‚úÖ 2. Kubelet ko restart karo
Node par SSH karke:
sudo systemctl restart Kubelet
80% cases me kubelet restart se node Ready ho jaata hai.

‚úÖ 3. CNI (aws-cni / calico) check karo
Network plugin girne se node NotReady ho jata hai.
Shellkubectl -n kube-system get pods | grep cnikubectl -n kube-system logs <aws-cni-pod>Show more lines

‚úÖ 4. Disk full? Clean karo
Disk 100% ho jaye to kubelet hang ho jata hai.
Shelldf -hdocker system prune -afsudo journalctl --vacuum-size=100M

‚úÖ 5. Node API server se connect ho raha hai?
Security group, network route table, ENI check karo.
Agar node API server ko hit nahi kar pa raha ‚Üí NotReady.

‚úÖ 6. Container runtime restart
sudo systemctl restart containerdShow more lines
Ya
sudo systemctl restart dockerShow more lines

‚úÖ 7. Final fix ‚Äî reboot or replace
Shellsudo rebootShow more lines
Aur agar phir bhi issue na solve ho:
Node ko terminate kar do.
EKS managed node group automatically naya healthy node bana dega.


------------------------------------------------------------------------

Developer Pushes Code ‚Üí GitHub
             |
             | (Webhook triggers)
             ‚Üì
         Jenkins / GitLab CI
             |
       (Maven/Gradle Build)
             |
      Creates JAR/WAR/Image
             |
             ‚Üì
       JFrog Artifactory
             |
     Deploy to Server/Cloud


     ------------------------------

     üõ† --Maven
Purana but reliable builder. Project ko compile/test karke output deta hai.
XML config hoti hai (pom.xml).

‚ö° Gradle
Maven ka fast version. Flexible + modern.
Groovy/Kotlin config hoti hai (build.gradle).

#### JUnit   (unit testing)

JUnit is a testing framework for Java.
It helps developers write unit tests to check if their Java code is working correctly.

-----------------------
üè≠ JFrog Artifactory
Build ke baad jo JAR/WAR/Docker image banta hai, woh yahan store hota hai.
Like ‚Äî software warehouse.
------üëâ JUnit tests the OUTPUT
üëâ SonarQube tests the QUALITY & SAFETY

------
FOR integration testing --
Tests multiple components together
(database + backend + API + services).
, we using POSTMAN CLI (newman)

Unit Tests (JUnit) ‚Üí logic is safe
2Ô∏è‚É£ API Tests (Newman) ‚Üí endpoints are safe
This validates that different components‚Äîlike API + DB‚Äîwork well together.‚Äù
------

üîî Webhook
Doorbell jaisa system.
Event hote hi ek system dusre ko automatically notify karta hai.
Jaisa:
GitHub ne push dekha ‚Üí Jenkins ko call kar diya ‚Üí Pipeline start.


------------------------------------------------------------------------

CloudFront ka EU region me latency mostly 4 reasons ki wajah se hoti hai ‚Äî cache thanda 
(cold cache), origin slow, caching galat set ki hui, ya network/routing issue. 
Pehle main CloudFront ka cache-hit ratio check karta hoon, phir origin (S3/ALB/EC2) 
ki response time dekh Leta hoon. Fir ensure karta hoon ki EU ke closest 
edge locations use ho rahe ho. Next, HTTP/3 enable karta hoon, TTLs sahi 
karta hoon, Price Class ko All kar deta hoon, aur Origin Shield enable 
karta hoon taaki caching better ho. Main ye bhi check karta hoon ki WAF 
rules latency bada to nahi rahe. Agar problem phir bhi rahe, to CloudFront 
RUM se real user latency check karta hoon aur 
AWS Global Accelerator use karke routing fast kar deta hoon.‚Äù


Cold cache ka matlab hai CloudFront ke edge servers ke paas aapki file pehle 
se store nahi hai. Isliye jab user request karta hai, CloudFront ko pehle origin 
(S3/ALB/EC2) se 
data lana padta hai ‚Äî jisse time zyada lagta hai.

-------
Origin slow ka matlab hai ki aapka backend ‚Äî jaise S3, EC2, ALB, API ‚Äî 
response dene me hi slow hai. Agar origin hi slow hai, to CloudFront chahe 
jitna fast ho, latency fir bhi badh jaati hai.

Common causes:

EC2 CPU high
Database slow
ALB response time badh gaya
S3 latency from distant region
Heavy code processing


-----
GitHub ‚Üí Webhook ‚Üí Jenkins ‚Üí Build & Compile (maven &gradle) ‚Üí Unit Tests ‚Üí Integration Tests 
‚Üí SonarQube Scan ‚Üí JFrog Artifactory ‚Üí Deployment


######CI-CD############# PIPELINE FLOW
Pehle main apna source code GitHub pe 
maintain karta hoon, jahan se main GitHub Webhook configure karta hoon taaki jab 
bhi koi developer code push kare, Jenkins pipeline automatically trigger ho jaaye. 
Jenkins trigger hote hi sabse pehla stage hota hai build and compile, jisme Maven 
ya Gradle code ko compile karke JAR/WAR generate karta hai. Build ke baad main 
Unit Testing chalata hoon JUnit ya TestNG se, taaki individual methods ya 
logic sahi kaam kar rahe ho. Uske baad Integration Testing hoti hai jisme main 
Newman/Postman ka use karta hoon, jo ensure karta hai ki API, 
database, aur services ek‚Äëdusre ke saath smoothly work karein. Phir main 
SonarQube scan run karta hoon taaki code quality, bugs, vulnerabilities, 
aur security issues detect ho sakein ‚Äî agar SonarQube Quality Gate fail ho 
jaata hai, toh pipeline deploy nahi hoti. Jab sab checks pass ho jaate hain, 
toh main final build artifact ko JFrog Artifactory mein upload karta hoon for 
versioned, centralized storage. Last stage mein Jenkins artifact ko deploy karta hai ‚Äî 
jaise AWS EC2, Tomcat, WebSphere, Docker ya Kubernetes ‚Äî taaki application production 
ya staging environment mein smoothly run ho sake. Overall yeh complete end‚Äëto‚Äëend 
CI/CD pipeline automated, secure, aur reliable deployment ensure karti hai.‚Äù

Unit test = logic check
Integration test = system check
SonarQube = quality + security
JFrog = artifact storage
Deployment = final release



########
 ‚ÄúCPU/Memory Normal But App Slow‚Äù
Most common root causes:

DB slow (80% cases)   (Throttling issue)
ALB latency
Disk I/O  (gp2 to gp3)
Thread/connection pool
Slow dependencies
Network latency
Incorrect Auto Scaling triggers
TargetResponseTime (agar yeh high ‚Üí app slow)


#### Canary

Canary Deployment ek release strategy hai jisme new version ko ek chhote ‚Äúsubset‚Äù 
of users ke liye roll out kiya jata hai, baaki users purane version 
(stable prod) pe hi rehte hain.
Agar new version stable lagta hai ‚Üí rollout expand.
Agar issues milte hain ‚Üí rollback easy & fast.

üéØ One‚ÄëLine Definition

Canary Deployment = New release ko 1‚Äì5% traffic par test karna, phir
gradually 10% ‚Üí 25% ‚Üí 50% ‚Üí 100% traffic par rollout karna.

## POD to AWS service
OIDC provider ‚Üí IAM trust policy/role ‚Üí ServiceAccount ‚Üí Pod


------------------------------------------------------------------------

CloudTrail = Audit logs of API calls  ( (full history))
[eg:- üé• Records every action:
‚ÄúUser X opened locker 32 at 3:12 PM from IP A.B.C.D‚Äù ]

example :-  EC2 stopping
-----------------------------------------------------------------------
EventBridge = Real-time events for automation  (only recent events)
üîî Alerts in real time:
‚ÄúLocker 32 was opened ‚Äî send SMS!‚Äù

example :- ec2:StopInstances called by user

---------------------------------------------------------------

üîîüîîEventBridge ‚Üí SQS ‚Üí Lambda ‚Üí S3 archive pipeline, which is the standard 
AWS pattern to store EC2 (or any AWS) events for long-term retention.üîîüîî

EventBridge cannot directly store events in S3.
So Lambda(Processes and uploads event to S3) acts as a bridge between them.


After Maven completes the build and tests, we execute mvn sonar:sonar to send 
the code, test results, coverage, and code quality metrics to SonarQube. Maven automatically 
uses the internal SonarScanner plugin, so no extra installation is needed.

We typically maintain around 70‚Äì80% code coverage using JaCoCo integrated with Maven
JaCoCo is a code coverage tool that generates coverage reports during Maven tests.

Our Quality Gate requires at least 70% coverage.
If coverage goes below that, the pipeline fails.

‚úÖ 1. What is pom.xml?
POM = Project Object Model
pom.xml is the main configuration file of a Maven project.
Isme hota hai:

Project dependencies (JARs you need)
Plugins (JaCoCo, Sonar, Surefire, etc.)
Build instructions
Version, groupId, artifactId
Repositories
Profiles


CloudTrail = Records events (logging)
EventBridge = Reacts to events (automation)

CloudTrail records all API activity for auditing and security.

Simple Example (VERY IMPORTANT)
‚ùå CloudTrail Example
CloudTrail log (after the fact):
User Anmol called ec2:TerminateInstances at 10:32 AM

No action taken. Just stored.

‚úî EventBridge Example
EventBridge can say:
If EC2 instance is terminated ‚Üí trigger SNS ‚Üí send alarm

Updated CICD 

GitHub
 Webhook ‚Üí Jenkins
1. Checkout code
2. Build & Compile (Maven / Gradle)
3. Unit Tests (JUnit)
4. Coverage (JaCoCo)
5. SonarQube (SAST)
6. SCA (Trivy)
7. Deploy to Dev
8. Integration Testing (Newman)
9. DAST (Invicti)


-----
Trivy ek security scanner hai jo aapke project ke:

Libraries
Dependencies
Docker images
OS packages
SBOM

‚Ä¶sab check karta hai vulnerabilities ke liye.
‚≠ê Simple language mein:

‚ÄúTrivy bata deta hai ki aapke project ke andar jo open-source libraries 
use ho rahi hain unme koi known security bug (CVE) toh nahi hai.‚Äù

Trivy ‚Üí code aur libraries scan karta hai
Invicti ‚Üí running website/API ko scan karke security holes dhoondta hai
‚≠ê Invicti kya detect karta hai?

SQL Injection
XSS
Authentication flaws
Authorization bypass (IDOR)
Insecure cookies


######################
1. Cosign image signing + OPA/Kyverno validation
2. Velero for EKS backups
##################################

1Ô∏è‚É£ Cosign Image Signing + OPA/Kyverno Validation ‚Äî (Supply Chain Security ka core funda)
üî• Problem kya hota hai?
Jab tum Docker image build karke ECR/DockerHub/JFrog me push karte ho‚Ä¶
koi guarantee nahi hoti ki image kisi ne tamper nahi kiya,
ya woh trusted source se aayi hai.
Ispe attacks hote hain:

Image replace ho jaye
Tag spoofing (fake v2.0 image)
Malicious image inject ho jaye


üü¢ Cosign kya karta hai? (Simple Explanation)
Cosign ek tool hai jo Docker image ko digitally sign karta hai.
Jaise Aadhaar me fingerprint hota hai ‚Üí identity confirm karta hai‚Ä¶
same waise Cosign image ka identity proof banata hai.
How it works:

Tum Docker image build karte ho
Tum Cosign se us image ko sign karte ho
Signature ECR ke ‚Äúsignature store‚Äù me save hota hai
Jab EKS me deploy hoti hai ‚Üí cluster check karta hai
‚ÄúIs image ka signature valid hai?‚Äù

Agar valid ‚Üí deploy
Agar invalid/unsigned ‚Üí deployment block üòé

üü£ OPA / Kyverno kya karte hain?
OPA (Open Policy Agent) ya Kyverno Kubernetes ke ‚Äúpolice officer‚Äù hote hain.
Har image cluster me deploy hone se pehle admission controller usko check karta hai.
Example policy:

‚ÄúONLY signed images allowed‚Äù
‚ÄúrunAsNonRoot = true hona chahiye‚Äù
‚Äúno image with tag :latest allowed‚Äù
‚ÄúImages must come from ECR only‚Äù

Combined flow:
Cosign ‚Üí image sign karta hai  
Kyverno/OPA ‚Üí check karta hai ki image signed hai ya nahi  

Ye pure flow ko bolte hain Supply Chain Security.

üéØ Final Summary (1)
‚úî Cosign = signature banata hai
‚úî OPA/Kyverno = signature verify karke only trusted image deploy karte hain
Isse pipeline enterprise‚Äëgrade secure ban jati hai.

------------------------------

3Ô∏è‚É£ Velero for EKS Backups ‚Äî Kubernetes ka Time‚ÄëMachine
üî• Problem kya hai EKS me?
EKS me ye sab Kubernetes cluster me stored hota hai:

Deployments
ConfigMaps
Secrets
Services
Ingress
Persistent Volumes (EBS)

Agar cluster corrupt / accidental delete ho jaye ‚Üí
Koi backup nahi to poora infra dobara banana padega.

üü¢ Velero kya karta hai?
Velero = Kubernetes ka backup + restore tool.
Velero can back up:
‚úî Entire namespaces
‚úî YAML manifests (deployments, services, ingresses, secrets‚Ä¶)
‚úî Persistent Volume snapshots (EBS snapshots)
‚úî Cross-region copy
‚úî Schedule-based automatic backups
Example:
Tum bolte ho:
‚ÄúHar raat 1 baje dev cluster ka backup le lo
aur EBS volume ka snapshot S3 me store karo
aur ek copy dusre region me bhejo.‚Äù
Velero will do it automatically.

üü£ Restore scenario:

Cluster crash
Migration
Cross-region DR
Accidental delete

1 command:
velero restore create --from-backup my-backup

And poora cluster wapis aa jata hai ‚Äî
including PV + apps + configs.
---------------------------------------------------------------------
####Falco ‚Üí Runtime scanning (jab pod chal rha hota hai) ###

## my CI/CD ##

Dev environment

Code
Github
Webhook
Gitlab (CI/CD)
Build & compile (maven & gradle)
Unit Testing (Junit)
JaCoCo(code coverage)
Sonarcube (sonarscanner + Quality gate)
get the artifact ( abc.jar)
Build the image
Perform SCA (Trivy) 
run the image in EC2 stadalone dev server for testing
Perform integration testing (NEWMAN ) (POSTMAN CLI)


Staging env

Perform Trivy (Full /strict scan)
Perform integration testing (NEWMAN ) (POSTMAN CLI) (OPTIONAL : AGAIN) (Full SCAN)
Push artifact to Jfrog
Sign the image (abc.jar) with Cosign 
Verify with Kyverno
Perform DAST (Invicti) (Final stage before pre production)

Production

push image from ECR to EKS 
Add Services (NODEPORT , ALB) (Target Groups)
Cluster autoscaler
Persident volume
HPA (for pod scalaibility)
IRSA for service accounts also
Put ALB domain in cloudfront
And use cloudfront Alias in route 53 hosted zone (Use Failover & Cross zone routing)
Also store secertsin AWS KMS 

For Alerting & Logs
Falco (For collectiong logs)
Node logs (Cloudwatch)
EKS Logs (prometuehs and Grafana)
Events (Cloudtrail to Eventbridge to SQS to Lambda fn to SNS/Dynamodb)

For backup & DR

Use AWS backup for Ec2 snapshot (Timely backup)
Pods running on ec2 but backend is on Oracle RDS (Cross zone replica allow)
Store datanbase date in S3 (lifecycle rules ( IA to glacier)



#############################################
----------------------------------------------------------------------------
