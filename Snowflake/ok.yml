kind: Pod                              
apiVersion: v1                     
metadata:                           
  name: testpod                  
spec:                                    
  containers:                      
name: c00                     
      image: ubuntu              
      command: 
  restartPolicy: Never         # Defaults to Always


--MULTICONTAINER--

apiVersion: v1
kind: Pod
metadata:
  name: anmol
  annotations:
    description: "This is the updated one"
spec:
  containers:
  - name: anmol
    image: busybox
    command: ["sh", "-c", "echo 'hello this is anmol' && sleep 5"]
  - name: abhishek
    image: busybox
    command: ["sh", "-c", "echo 'hello this is abhishek' && sleep 5"]
  restartPolicy: Never


-- kubectl logs anmolabhi -f abhishek
-- kubectl exec env -c anmol -it -- sh

--ENVIRONAMENT VARIABLES--

apiVersion: v1
kind: Pod
metadata:
  name: env
spec:
  containers:
  - name: anmol
    image: busybox
    command: ["sh", "-c", "echo 'This is env' && sleep 360"]
    env:
    - name: mynameC
      value: Anmol

    for checking:-  echo $myname

    -- PORTS containers:--

apiVersion: v1
kind: Pod
metadata:
  name: testpod4
spec:
  containers:
    - name: c00
      image: httpd
      ports:
        - containerPort: 80

--- MULTI PODS--

apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
    - name: pod1
      image: httpd
      ports:
        - containerPort: 80

---
apiVersion: v1
kind: Pod
metadata:
  name: pod2
spec:
  containers:
    - name: pod2
      image: busybox
      command: ["sh", "-c", "echo 'Hi this is multipod' && sleep 500"]

   
------ LABELS-----
    
    kubectl label pod anmol class=higher     (label banana ouside manifest yml)
    kubectl get pods --show-labels           (current labels ki info )
    kubectl get pods -l env=production       (give the pod which has that label)  

------ Labels & Selectors------

Equality based (=, !=)
Set based (in, notin, exists)  [Multiple values]

get pods -l 'name in (a1,a2)'
get pods -l 'name notin (a1,a2)'


apiVersion: v1
kind: Pod
metadata:
  name: anmol
  labels:
    name: a1
    env: production
    company: slk
spec:
  containers:
    - name: myapp
      image: busybox
      command: ["sh", "-c", "echo 'Hi this is my label' && sleep 500"]


------- NODE SELECTORS-------
 
1 master has 4 nodes (if we want to create on specific node then we use node selector)
e.g   in labels (hardware=t2.medium  )  [So that node have t2.medium instance vha pod create honge]

    command:  kubectl label nodes (private ip of node) hardware=t2.micro
              kubectl describe pod anmol

--IF want to add in code--- (It comes under spec)

apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: mycontainer
      image: busybox
  nodeSelector:
    disktype: ssd


------ Scaling & Replications ------

1. Reliability  (Agar ek pod kaharab hoga , to doosre par shift ho jaayega[]replica)
2. Scaling (agar load bdh rha hai (suppose 3 pod hai) , to load bdthe time POD auto create ho jaayege node pe)
3. Load Balancers (Multiple PODS ko balance krna , equally)
4. Rollimng Update

Suppose 1 Node{500 POD hai or utne hi honge } agar need jyda hai to automnatically (new nodes bengi or unpar baaki 
podes create honge)

5. ReplicationController =
      Suppose, We decsribe our desired state =2 (agar ek fail hogye , to ek or create honge)
      **agar ek delete hogya , fir bhi 2 hi honge ** (bcz we put desired state =2)

apiVersion: v1
kind: ReplicationController
metadata:
  name: myreplica                 [Object name]
spec: 
  replicas: 2                    [Ye hamara desired state hai, means itne honge hi , if one got deleted]  [POD creation]
  selector:
    hardware: t2.medium               [ POD vhi create honge jo node t2. medium par hoga]
  template:                          {POD kaisa bngea ye batayega}
    metadata:
      name: mypod
      labels:
        myname: Anmol                 [Lables]
        env: production
    spec:
      containers:
      - name: c1
        image: busybox
        command: ["/bin/bash", "-c", "while true; do echo Hello; sleep 5; done"]

kubectl describe rc myreplica

kubectl scale --replicas=4 rc -l myname=Anmol   (jha par label mynam=anmol hai us pod ko 4 baar  banaodo)
                                                 (2 phele bnaaye the ab total 4 hogye)
kubectl scale --replicas=1 rc -l myname=Anmol    (DOWN krna)

***Replica_Set** (equlaity based as well as set based dono par kaam krta hai)
means (agar scaling krni ho to apn labels par multiple condition daal skte hai 

like :- kubectl scale rs myrs --replicas=1

Selector = ‚ÄúWhich Pods should I manage?‚Äù
Template = ‚ÄúWhat should my Pods look like?‚Äù

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myrs
spec:
  replicas: 2
  selector:
    matchExpressions:
      - key: myname
        operator: In
        values:
          - Anmol
          - Abhi
      - key: env
        operator: NotIn
        values:
          - production
  template:
    metadata:
      name: testpod7
      labels:
        myname: Anmol
        env: staging
    spec:
      containers:
        - name: c2
          image: ubuntu
          command: ["/bin/bash", "-c", "while true; do echo Technical-Guftgu; sleep 5; done"]


***************Deployment_ROLLBACK********

Deployment provides decraltive set for Pods & Replicasets

             Deployment ---------
             |                   |
             replica set1         replicaset2
             |                        |
             POd1                     Pod 2

             agar Pod1 me kuch update hoga to deplpoyemnt object ek nya replica bna dega (old wale pod stop/delete
              ho jaayege)
             or agar rollback krna ho Pod2 to Pod 1 (ROLLBACK object kr dega)
    saara code Pod2 kaa pod 1 me aajyega (blhe hi dono me no. of pods alag ho)
    Scaling bhi kr skte hai 

    jo pod sbe aakhir me create honge vo sbse phele delete honge
    We can pause the depolyment(after fixing we can resume)

kubectl set image deployment/mydeployments c4=busybox:latest (image ko update krne ke liye)

**kubectl get deploy   (jab ko apna deployment cluster me dekhna/inspect krna ho)
                      (Name, age, ready, up to date .... ye sab information dega aapko aapke deploy kaa)

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydeployments
spec:
  replicas: 2
  selector:
    matchLabels:
      name: deployment
  template:
    metadata:
      labels:
        name: deployment
    spec:
      containers:
        - name: c4
          image: busybox
          command: ["/bin/bash", "-c", "while true; do echo Hello; sleep 5; done"]

kubectl describe deploy mydeployments (kitne rs hai pods hai)
kubectl get rs (kitne replica sets hai)


**Rollout_Commands:-
kubectl rollout status deployment mydeployment
kubectl rollout history deployment mydeployment {Kitni baar changes kiye hai, vo sab aajyega}
kubectl rollout undo (previous version par aajeyga replica)
kubectl rollout undo deployment/<name> --to-revision=<n> (specifix version)
-------------------------------------------------------------------------------------------------
**ReplicaSet ‚Äì Complete Bullet Points
üîπ What is a ReplicaSet?

Pods ko desired count me maintain karta hai (e.g., always 3 Pods running).
Agar koi Pod crash ho jaye ‚Üí ReplicaSet naya Pod bana deta hai.
ReplicaSet = Pod lifecycle manager.

üîπ Why we don‚Äôt use ReplicaSet directly?

Direct ReplicaSet me:
‚ùå Rolling updates nahi
‚ùå Rollback nahi
‚ùå Version history nahi
Deployment hi in sab ko automatically manage karta hai.

üîπ How ReplicaSet is used today?

**Mostly Deployment ke through indirectly use hota hai.
**Deployment har update par new ReplicaSet create karta hai.


‚≠ê How Deployment Uses ReplicaSet Internally (Bullet Points)
üîπ 1. Deployment created ‚Üí first ReplicaSet created

Aap YAML apply karte ho ‚Üí Deployment ‚Üí ReplicaSet v1 banata hai ‚Üí Pods v1 run hote hain.

üîπ 2. You update Deployment ‚Üí new ReplicaSet

Image update, env update, config change = ReplicaSet v2 create hota hai.
Pods v2 rollout hote hain.

üîπ 3. Rolling update = two ReplicaSets

Old ReplicaSet (v1) ‚Üí scale down
New ReplicaSet (v2) ‚Üí scale up
Zero downtime process.

üîπ 4. Deployment never touches Pods directly

Deployment ‚Üí ReplicaSet ‚Üí Pod
(Indirect control chain)


‚≠ê Rollout ‚Äì Easy Bullet Points (Hinglish)
üîπ What is rollout?

‚ÄúRollout‚Äù means deployment ka new version apply karna ‚Äî new pods create hona.

üîπ What triggers a rollout?

Image change
Env var change
Label change
Config update
Pod template me koi bhi change

üîπ Rollout process

Deployment creates new ReplicaSet
Slowly new pods start
Old pods stop/delete
No downtime if strategy = RollingUpdate

üîπ Commands

Check rollout status
kubectl rollout status deployment <name>

See rollout history
kubectl rollout history deployment <name>


‚≠ê Rollback ‚Äì Easy Bullet Points (Hinglish)
üîπ What is rollback?

Rollback = deployment ko previous stable version par wapas lana.
Deployment old ReplicaSet ko activate kar deta hai.

üîπ When rollback works?

Jab old ReplicaSet exist karta ho
Deployment ke paas atleast 1 revision stored ho
Pod template previously change hua ho

üîπ When rollback does NOT work?

No old ReplicaSet created (first deployment)
You never updated pod template
old ReplicaSet manually delete kar diya
revisionHistoryLimit: 0 set ho

üîπ Rollback command
kubectl rollout undo deployment <name>Show more lines
üîπ What happens internally?

Old ReplicaSet scale-up
New ReplicaSet scale-down
Old version ke pods wapas create ho jate hain


‚≠ê Ultra-Simple Summary

ReplicaSet = Pods ko maintain karne wala worker
Deployment = Boss ‚Üí ReplicaSet create/update/delete karta hai
Rollout = New version deploy
Rollback = Old version par wapas jana
Deployment ke bina ReplicaSet = No rolling update, no rollback
Isliye hum Deployment hi use karte hain, ReplicaSet manually nahi.

-------------------------------------------------------------------------------------------------
-------NETWORKING-----

apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
    - name: c00
      image: busybox
      command: ["/bin/bash", "-c", "while true; do echo Anmol; sleep 5; done"]
    - name: c01
      image: httpd
      ports:
        - containerPort: 80

Testing (Con1 to cont2  inside same POD same NODE)
[root@ip-172-31-89-65 ec2-user]# kubectl exec pod1 -it -c c00 -- /bin/sh
/ # curl localhost:80
/ # wget -qO- http://localhost:8

Testing (Pod to Pod inside same node)

[root@ip-172-31-89-65 ec2-user]# kubectl get pods -o wide
NAME              READY   STATUS    RESTARTS   AGE     IP            NODE                       NOMINATED NODE   READINESS GATES
myreplica-h99h6   1/1     Running   0          40m     10.244.0.8    my-cluster-control-plane   <none>           <none>
pod4              1/1     Running   0          6m47s   10.244.0.11   my-cluster-control-plane   <none>           <none>
pod5              1/1     Running   0          19m     10.244.0.10   my-cluster-control-plane   <none>           <none>

# use ip address of different pods to communicate b/w pods
# curl ipaddress 

-------SERVICES---------

# tumhara pod ip1 par chl rha tha , par kuch issue hua pod kharab hogya or replica set ki need se 1 or pod bn gya
# uss case me POD ki Ip change hoti hai we know...

yha par hum SERVICEs object ko replica set ke upar lgaate hai (Virtual IP provide krta hai) (MAP krta hai replica ko POD ko)
# so agar POD ki Ip change hoti hai to bhi Hmm usko SERVICES ki IP se access kr skte hai, respose same hoga
# replica ke alawa bhi or bhi object par lgaa skte hai , e.g deployment bla bla (static IP dega SERVICES )
# Inshort (SERVICES ek bridge hai PODS and end user ke beech , jo virtual ip deta hai)
# Mapping kaa kaam kube proxy krta hai (virtual ip map to Pod Ip)
# Cluster Ip (default) , Elastic Ip and Load balancers  (Ye treeke hote hai services ke)
## 2 nodes in one cluster each node has one pod (If they want to communicate with other then thye use Cluster Ip)
# Cluster Ip ke upar Node Port lgta hai or fir uske upar Load balancer lgta hai
# cluster ke bahar IP use nhi hoti

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydeployments
spec:
  replicas: 1  
  selector:                                      # tells the controller which pods to watch/belong to
    matchLabels:
      name: deployment
  template:
    metadata:
      name: testpod1
      labels:
        name: deployment
    spec:
      containers:
        - name: c00
          image: httpd
          ports:
            - containerPort: 80

====================
apiVersion: v1
kind: Service
metadata:
  name: demoservice
spec:
  selector:
    name: deployment  # Targets pods with this label
  ports:
    - port: 80         # Port exposed by the service
      targetPort: 80   # Port on the pod
  type: ClusterIP      # Default type, internal access only
                     

[root@ip-172-31-89-65 ec2-user]# kubectl get svc
NAME                  TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
demoservice           ClusterIP      10.96.112.212   <none>        80/TCP         61s

----NODEPORT-----
## U can access your pod outside cluster by public dns with port 
# We dont need ip on this , by port we can access (Port map krna hota hai virtual IP ko(cluster ip ko))

apiVersion: v1
kind: Service
metadata:
  name: demoservice
spec:
  selector:
    name: deployment  # Targets pods with this label
  ports:
    - port: 80         # Port exposed by the service (client uses this)
      targetPort: 80   # Port on the pod (container port (app listens here))
  type: NodePort

  service/demoservice configured
[root@ip-172-31-89-65 ec2-user]# kubectl get svc
NAME                  TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
demoservice           NodePort       10.96.112.212   <none>        80:32424/TCP   45h

We got port above for nortport
kubectl describe svc demoservice (get the details about Nodeport)

------------------------------------------------------------------------
‚≠ê 1. ClusterIP (Internal‚Äëonly Service)
üîπ Simple meaning:
Service sirf cluster ke andar accessible hota hai ‚Äî bahar se koi access nahi.
üîπ You use it when:

Microservices ek‚Äëdusre se baat kar rahe hote hain
Example: backend ‚Üí database, frontend ‚Üí backend (within cluster only)

üîπ Key points:

Default service type in Kubernetes
Best for internal communication
Fast and secure because it never exposes anything to outside world


‚≠ê 2. NodePort (Basic External Access)
üîπ Simple meaning:
Cluster ke har node par ek public port open hota hai
(30000‚Äì32767 range), jisse service access hoti hai.
üîπ You use it when:

You want simple external access
No cloud load balancer available
Dev/test environment me quick access chahiye

üîπ Key points:

Access format: NodeIP : NodePort
Exposes service to outside world
Internally still uses ClusterIP (NodePort ‚Üí ClusterIP ‚Üí Pod)
Not ideal for production load (limited features)


‚≠ê 3. LoadBalancer (Production‚Äëgrade External Access)
üîπ Simple meaning:
Cloud provider (AWS, GCP, Azure) ek public Load Balancer create karta hai
jo traffic ko pods tak distribute karta hai.
üîπ You use it when:

Production application expose karna ho
Need: public IP + traffic balancing + reliability
Multiple pods handle heavy traffic

üîπ Key points:

Internet ‚Üí LoadBalancer ‚Üí NodePort ‚Üí ClusterIP ‚Üí Pods
Automatically creates a public IP
Best for large-scale and real-world applications
Requires cloud infrastructure (not for bare-metal without extra setup)


üîó Relationship

ClusterIP is the base.
NodePort builds on ClusterIP for external access.
LoadBalancer builds on NodePort for cloud-based load balancing.

------------------------------------------------------------------------
-------VOLUME------

# volukme POD par connected hota hai
# agar container delete hota hai, to volume par farak nhi pdta
# new container par same volume aajayega
# only inside POD volume accessible hota hai

# Ek POD me volume ko multiple containers ek saath attach kr skte hai

----- VOLUME TYPES---
nfs , elastic block storage, gitrepo

--- Empty DIR---

# agar POD delete/crash hogya to
# new POD bnane par , empty volume create hogi

# Hum mount krge volume ko different containers par, jo bhi update hoga sbka sab volume par data jaayega

apiVersion: v1
kind: Pod
metadata:
  name: myvolemptydir
spec:
  containers:
    - name: c1
      image: busybox
      command: ["/bin/sh", "-c", "sleep 15000"]
      volumeMounts:
        - name: xchange
          mountPath: "/tmp/xchange"
    - name: c2
      image: busybox
      command: ["/bin/sh", "-c", "sleep 10000"]
      volumeMounts:
        - name: xchange
          mountPath: "/tmp/data"
  volumes:
    - name: xchange
      emptyDir: {}

## We can access same data via two containers

[root@ip-172-31-89-65 ec2-user]# kubectl exec myvolemptydir -c c1 -it -- /bin/sh
/ # ls
bin           etc           lib           proc          product_uuid  sys           usr
dev           home          lib64         product_name  root          tmp           var
/ # cd tmp
/tmp # ls
xchange
/tmp # cd xchange
/tmp/xchange # ls -ltr
total 0
/tmp/xchange # touch abc.txt
/tmp/xchange # cd 
~ # exit -------------------------------------------------------------------------------------------------
[root@ip-172-31-89-65 ec2-user]# kubectl exec myvolemptydir -c c2 -it -- /bin/sh
/ # ls
bin           etc           lib           proc          product_uuid  sys           usr
dev           home          lib64         product_name  root          tmp           var
/ # cd tmp
/tmp # 'ls

/tmp # ls
data
/tmp # cd data
/tmp/data # ls -ltr
total 0
-rw-r--r--    1 root     root             0 Jul 23 23:33 abc.txt


------- HOSTPATH----

# Ek POD ko doosri POD ke volume ko access krna
# humko host path me mount krna hota hai

apiVersion: v1
kind: Pod
metadata:
  name: myvolhostpath
spec:
  containers:
    - name: testc
      image: busybox
      command: ["/bin/sh", "-c", "sleep 15000"]
      volumeMounts:
        - name: testvolume
          mountPath: /tmp/hostpath
  volumes:
    - name: testvolume
      hostPath:
        path: /tmp/data
        type: DirectoryOrCreate

        -------------------------------------------------------------------------------------------------

----- Persistent Volume ----

# Cluster wide resource
# Multiple woker nodes PV ko access kr skte hai (type of NAS)
# POD ke delete hone par kuch farak nhi pdega
# We create EBS  and we connect to node (Its always available)

## **EBS ko divide krge volumes me or nodes ko dege**
# Suppose 100 gb hai EBS , and jitni volume chaaiye vo hum pod par define krege (like 5gb or 6gb)
# PV objects dene hote hai , usme se distribute hote hai
# Shared volume accross cluster 
# Claim krna pdta hai , kitni volume chaaiye
# Hum release bhi kr skte hai

# phele volume define krge , then claim krege usme se kitni chaaiye jo define kri hai

----VOLU DEFINE--

apiVersion: v1
kind: PersistentVolume
metadata:
  name: myebsvol
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  awsElasticBlockStore:
    volumeID: vol-042dda85865440f2c # YAHAN APNI EBS VOLUME ID DAALO
    fsType: ext4

------CLAIM KRNA---

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myebsvolclaim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

[root@ip-172-31-89-65 ec2-user]# kubectl apply -f defvol.yml
persistentvolume/myebsvol created
[root@ip-172-31-89-65 ec2-user]# kubectl get pv
NAME       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
myebsvol   1Gi        RWO            Recycle          Available                          <unset>     
                     12s
[root@ip-172-31-89-65 ec2-user]# vi claimvol.yml
[root@ip-172-31-89-65 ec2-user]# kubectl apply -f claimvol.yml
persistentvolumeclaim/myebsvolclaim created
[root@ip-172-31-89-65 ec2-user]# kubectl get pvc
NAME            STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   VOLUMEATTRIBUTESCLASS   AGE
myebsvolclaim   Pending         

----Attached Into PODS----

apiVersion: apps/v1
kind: Deployment
metadata:
  name: pvdeploy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mypv
  template:
    metadata:
      labels:
        app: mypv
    spec:
      containers:
        - name: shell
          image: busybox
          command: ["bin/sh", "-c", "sleep 10000"]
          volumeMounts:
            - name: mypd
              mountPath: "/tmp/persistent"
      volumes:
        - name: mypd
          persistentVolumeClaim:
            claimName: myebsvolclaim

-------LivenessProbe----HEALTHCHECK------

# Container ki health check hoti hai(Ek time ke baad)
# if response 0 aayega means good  hai
# Otherwise POD ko regenerate kraayega (kubelet krega)
# Conatiner alive hai ke nhi, uske under application running hai ke nhi
--Parameters---
# Initial Delay dete hai (Like 5 sec , means after 5 sec test should be start)
# period second (eg 20 sec , means fater every 30 sec health check hoga)
# timeout second (e.g. agar 30 sec tak kuch bhi nhi ayaa to fail ho jaayega)

Liveness Probe

Purpose: Checks if the container is still alive (running).
If the liveness probe fails:

Kubernetes kills the container and restarts it (based on restart policy).
Use Case: Detect deadlocks or stuck processes.
------------------------------

Readiness Probe

Purpose: Checks if the container is ready to serve traffic.
If the readiness probe fails:

Kubernetes removes the pod from Service endpoints (no traffic sent).
Use Case: Wait until app finishes initialization or dependencies are ready.
------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: mylivenessprobe
  labels:
    test: liveness
spec:
  containers:
    - name: liveness
      image: busybox
      args:
        - /bin/sh
        - -c
        - touch /tmp/healthy; sleep 1000
      livenessProbe:
        exec:
          command:
            - cat
            - /tmp/healthy
        initialDelaySeconds: 5
        periodSeconds: 5
        timeoutSeconds: 30


# Path par jakar "echo $" dena hoga (agar 0 yaa means healthy hai, otherwise unhealthy)

-----------------CONFIG & SECERTS-----------------

# Virtual memory create krta hai or vha .conf file save krta hai
# agar koi POD kaharab hoga , to nye wale ke saath automatic aajeygi (jo path doge aap us par)
# configurtion maintain krt hai (plain text file hoti hai)
# e.g (Testing , QA or production  jab file jaati hai, comit hoke)
# jaise jaise move kregi (vaise vaise map hogi files)

# secerts (contains id, pass), sensitive data
# ***readable nhi hota
# encrypted hota hai  (txt or yaml file)

# Humko virtual memory ko map krna hota hai kisi path par


# Config or secerts 2 treh se access hota hai (volume in POD and Environment variable)

---CRASH LOOP BACK FIX--

To fix a CrashLoopBackOff issue in Kubernetes, i'll start by 
checking the pod logs using kubectl logs <pod-name> --previous to 
identify the root cause of the crash. If the problem is due to 
application errors, correct the code or configuration and redeploy. 
For missing environment variables or secrets, I'll ensure that they are properly 
defined in ConfigMaps or Secrets and referenced in the pod specifications. 
If resource limits are too strict and causing OOMKilled, I'll increase CPU 
or memory requests and limits in the deployment YAML. Also, I'll verify 
that liveness and readiness probes are correctly configured, 
because incorrect probes can cause unnecessary restarts. After making changes, 
apply the updated manifest using kubectl apply -f <file> and monitor the 
pod with kubectl get pods --watch to confirm it moves to a Running state. 

mera first step hamesha pod ke logs check karna hota hai using 
kubectl logs <pod-name> --previous, taaki mujhe exact pata chale container 
kis reason se crash ho raha hai. Mostly yeh issues application errors, 
missing configs. Agar logs me application‚Äëlevel bug ya incorrect configuration milti hai, 
toh main code ya config fix karke image redeploy kar deta hoon. Kabhi-kabhi CrashLoopBackOff 
missing environment variables ya Secrets ki wajah se bhi hota hai, toh main ensure 
karta hoon ki ConfigMaps/Secrets properly defined ho . Production me ek common reason 
OOMKilled bhi hota hai, jisme container memory limit cross kar deta hai. Aise case me main
 CPU/memory requests aur limits adjust kar deta hoon. Saath hi main liveness aur readiness 
probes bhi recheck karta hoon, kyunki galat probe configuration bhi container 
ko unnecessary restarts me daal deta hai. All fixes apply karne ke baad main 
kubectl apply se manifest update karta hoon aur kubectl get pods --watch se 
monitor karta hoon until pod stable Running state me aa jaye.

For OOM error:- kubectl top pod <pod-name> -n <namespace>

----- Load in PODS----

In Kubernetes, you can check the load on pods by monitoring their 
resource usage, primarily CPU and memory. The most common way is using 
the kubectl top pod command, which requires the Metrics Server to be 
installed in the cluster. This gives real-time resource consumption 
for each pod. Additionally, you can use kubectl describe pod <pod-name> 
to view resource requests and limits, though this shows configuration 
rather than live usage. For more detailed monitoring, tools like 
Prometheus with Grafana or CLI dashboards like K9s provide historical 
and real-time metrics. Kubernetes itself doesn‚Äôt measure load directly, 
so external monitoring solutions are often used for deeper insights.

In Kubernetes, hum pod ka load primarily uske CPU aur memory usage se 
measure karte hain. Quick real-time usage dekhne ke liye hum kubectl top 
pod use karte hain, jo tabhi kaam karta hai jab cluster me Metrics Server 
installed ho. Agar hume pod ki resource configuration samajhni ho‚Äîjaise 
requests aur limits‚Äîtab hum kubectl describe pod dekhte hain, but woh 
live usage nahi dikhata. Real production environments me teams usually 
Prometheus + Grafana use karti hain, jisse detailed dashboards, alerts,
 aur historical data milta hai. Overall, Kubernetes 
 directly ‚Äúload‚Äù nahi measure karta, isliye external 
monitoring tools load analysis ke liye essential hote hain.
AWS EKS me aap CloudWatch Container Insights bhi use kar sakte ho‚Äîye AWS ka native 
solution hai jo cluster, node, pod, aur container-level metrics deta hai.


------------------------------
Sonarcube---

SonarQube Jenkins ‡§Æ‡•á‡§Ç ‡§á‡§∏ ‡§§‡§∞‡§π ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§Ø‡§π ‡§Ü‡§™‡§ï‡•á ‡§ï‡•ã‡§° ‡§ï‡•ã 
bugs, vulnerabilities, ‡§î‡§∞ code smells ‡§ï‡•á ‡§≤‡§ø‡§è analyze ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ 
Jenkins pipeline ‡§Æ‡•á‡§Ç build ‡§î‡§∞ test ‡§ï‡•á ‡§¨‡§æ‡§¶, Jenkins SonarScanner 
(Maven, Gradle ‡§Ø‡§æ CLI ‡§ï‡•á ‡§ú‡§∞‡§ø‡§è) ‡§ö‡§≤‡§æ‡§§‡§æ ‡§π‡•à, ‡§ú‡•ã analysis results ‡§ï‡•ã 
SonarQube server ‡§™‡§∞ ‡§≠‡•á‡§ú‡§§‡§æ ‡§π‡•à‡•§ Server ‡§á‡§® results ‡§ï‡•ã Quality Gate 
‡§ï‡•á against evaluate ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, ‡§ú‡•ã ‡§§‡§Ø ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§ï‡•ã‡§° standards ‡§™‡§∞ 
‡§ñ‡§∞‡§æ ‡§â‡§§‡§∞‡§§‡§æ ‡§π‡•à ‡§Ø‡§æ ‡§®‡§π‡•Ä‡§Ç‡•§ ‡§Ö‡§ó‡§∞ Quality Gate fail ‡§π‡•ã ‡§ú‡§æ‡§§‡§æ ‡§π‡•à, ‡§§‡•ã Jenkins 
waitForQualityGate() step ‡§ï‡§æ ‡§á‡§∏‡•ç‡§§‡•á‡§Æ‡§æ‡§≤ ‡§ï‡§∞‡§ï‡•á pipeline ‡§ï‡•ã fail ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ 
‡§π‡•à‡•§ ‡§á‡§∏‡§∏‡•á ‡§Ø‡§π ‡§∏‡•Å‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§π‡•ã‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§ï‡•á‡§µ‡§≤ 
clean ‡§î‡§∞ secure code ‡§π‡•Ä ‡§Ü‡§™‡§ï‡•á CI/CD pipeline ‡§Æ‡•á‡§Ç ‡§Ü‡§ó‡•á ‡§¨‡§¢‡§º‡•á‡•§



Quality Gate ‡§ï‡§æ ‡§Æ‡§§‡§≤‡§¨ ‡§¨‡§π‡•Å‡§§ ‡§Ü‡§∏‡§æ‡§® ‡§π‡•à: ‡§Ø‡§π SonarQube ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§ö‡•á‡§ï‡§™‡•â‡§á‡§Ç‡§ü 
‡§π‡•à ‡§ú‡•ã ‡§§‡§Ø ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§Ü‡§™‡§ï‡§æ ‡§ï‡•ã‡§° ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à ‡§Ø‡§æ ‡§®‡§π‡•Ä‡§Ç‡•§ ‡§á‡§∏‡§Æ‡•á‡§Ç ‡§ï‡•Å‡§õ ‡§®‡§ø‡§Ø‡§Æ ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç ‡§ú‡•à‡§∏‡•á ‚Äì 
‡§ï‡•ã‡§à ‡§®‡§Ø‡§æ ‡§¨‡§°‡§º‡§æ bug ‡§®‡§π‡•Ä‡§Ç ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è, ‡§ï‡•ã‡§° ‡§ï‡§µ‡§∞‡•á‡§ú ‡§ï‡§Æ ‡§∏‡•á ‡§ï‡§Æ 80% ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è, 
‡§î‡§∞ ‡§ï‡•ã‡§à security issue ‡§®‡§π‡•Ä‡§Ç ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è‡•§ ‡§Ö‡§ó‡§∞ ‡§Ø‡•á ‡§®‡§ø‡§Ø‡§Æ ‡§™‡•Ç‡§∞‡•á ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç ‡§§‡•ã Quality 
Gate Pass ‡§π‡•ã‡§§‡§æ ‡§π‡•à, ‡§®‡§π‡•Ä‡§Ç ‡§§‡•ã Fail‡•§ Jenkins ‡§Æ‡•á‡§Ç ‡§Ö‡§ó‡§∞ Quality Gate fail ‡§π‡•ã ‡§ú‡§æ‡§è, 
‡§§‡•ã pipeline ‡§∞‡•Å‡§ï ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à ‡§§‡§æ‡§ï‡§ø ‡§ñ‡§∞‡§æ‡§¨ ‡§ï‡•ã‡§° deploy ‡§® ‡§π‡•ã‡•§


Ingress Controller ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•ã‡§§‡§æ ‡§π‡•à? (Kubernetes)
Short answer:
Ingress Controller Kubernetes cluster ‡§ï‡•á ‡§Ö‡§Ç‡§¶‡§∞ ‡§ö‡§≤‡§®‡•á ‡§µ‡§æ‡§≤‡§æ ‡§è‡§ï component ‡§π‡•à 
‡§ú‡•ã Ingress resources ‡§ï‡•ã ‡§™‡§¢‡§º‡§ï‡§∞ ‡§Ü‡§™‡§ï‡•á HTTP/HTTPS traffic ‡§ï‡•ã ‡§∏‡§π‡•Ä Service/Pod 
‡§§‡§ï route ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ‡§Ø‡§π ‡§Ö‡§ï‡•ç‡§∏‡§∞ reverse proxy / load balancer ‡§ï‡•Ä ‡§§‡§∞‡§π ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à 
‡§î‡§∞ TLS termination, path/host-based routing, ‡§î‡§∞ ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ/observability ‡§ú‡•à‡§∏‡•Ä ‡§∏‡•Å‡§µ‡§ø‡§ß‡§æ‡§è‡§Å ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§

‡§ï‡•ç‡§Ø‡•ã‡§Ç ‡§ö‡§æ‡§π‡§ø‡§è?

NodePort/LoadBalancer ‡§∏‡•á ‡§Ü‡§™ ‡§∏‡•Ä‡§ß‡•á service expose ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç, ‡§≤‡•á‡§ï‡§ø‡§®:

‡§π‡§∞ service ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ö‡§≤‡§ó external LB ‡§Æ‡§π‡§Å‡§ó‡§æ ‡§î‡§∞ ‡§ú‡§ü‡§ø‡§≤ ‡§π‡•ã ‡§ú‡§æ‡§§‡§æ ‡§π‡•à‡•§
Hostname ‡§Ø‡§æ path ‡§ï‡•á ‡§Ü‡§ß‡§æ‡§∞ ‡§™‡§∞ advanced routing ‡§Æ‡•Å‡§∂‡•ç‡§ï‡§ø‡§≤ ‡§π‡•à‡•§


Ingress + Ingress Controller ‡§ï‡•á ‡§∏‡§æ‡§•:

‡§è‡§ï ‡§π‡•Ä entry point (DNS) ‡§∏‡•á multiple apps serve ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§
foo.example.com ‡§î‡§∞ bar.example.com ‡§Ø‡§æ /api, /web ‡§ú‡•à‡§∏‡•á paths ‡§ï‡•ã ‡§Ö‡§≤‡§ó-‡§Ö‡§≤‡§ó services ‡§§‡§ï route 
‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§
TLS/Certificates centrally manage ‡§π‡•ã ‡§ú‡§æ‡§§‡•á ‡§π‡•à‡§Ç‡•§




Core Concepts

Ingress Resource: Kubernetes object ‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç rules ‡§≤‡§ø‡§ñ‡§§‡•á ‡§π‡•à‡§Ç (host, path, backend service+port).
Ingress Controller: ‡§µ‡§π software ‡§ú‡•ã ‡§á‡§® rules ‡§ï‡•ã ‡§™‡§¢‡§º‡§ï‡§∞ ‡§µ‡§æ‡§∏‡•ç‡§§‡§µ‡§ø‡§ï proxy/LB configure ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
Common Controllers:

NGINX Ingress Controller
Traefik
HAProxy
Cloud-specific: AWS ALB Ingress Controller, GKE Ingress, Azure Application Gateway Ingress, etc.




‡§Ø‡§π ‡§ï‡•à‡§∏‡•á ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à (Flow)

‡§Ü‡§™ ‡§è‡§ï Ingress YAML ‡§¨‡§®‡§æ‡§§‡•á ‡§π‡•à‡§Ç ‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç host/path routing rules ‡§π‡•à‡§Ç‡•§
Ingress Controller (cluster ‡§ï‡•á ‡§Ö‡§Ç‡§¶‡§∞ ‡§è‡§ï Deployment/DaemonSet) ‡§á‡§® rules ‡§ï‡•ã watch ‡§ï‡§∞‡§ï‡•á ‡§Ö‡§™‡§®‡•Ä 
internal config ‡§¨‡§®‡§æ‡§§‡§æ ‡§π‡•à‡•§
External traffic DNS/LB ‡§∏‡•á Controller ‡§§‡§ï ‡§Ü‡§§‡§æ ‡§π‡•à‡•§
Controller reverse proxy ‡§ï‡•Ä ‡§§‡§∞‡§π ‡§∏‡§π‡•Ä Kubernetes Service ‡§ï‡•ã request forward ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, 
‡§ú‡•ã Pods ‡§§‡§ï ‡§ú‡§æ‡§§‡§æ ‡§π‡•à‡•§
(Optional) Controller TLS terminate ‡§ï‡§∞ ‡§¶‡•á‡§§‡§æ ‡§π‡•à ‡§î‡§∞ HTTP ‚Üí Service ‡§§‡§ï plain traffic ‡§≠‡•á‡§ú‡§§‡§æ ‡§π‡•à‡•§


------------------------------------------------------------------------
Kubelet Kubernetes ka node agent hai jo pods ko start, stop, monitor karta hai aur node 
ko healthy state me maintain karne ka kaam karta hai.

Pod Pending ka matlab hota hai ki Kubernetes ne pod create toh kar diya, par uske liye 
koi suitable node nahi mil rahi scheduling ke liye

1. Check pending reason
Sabse pehle reason dekho:

kubectl describe pod <pod-name>
Yahan exact reason mil jaata hai ‚Äî jaise CPU/Memory shortage, taints, 
nodeSelector mismatch, PVC issues, etc.

‚úÖ 2. Not enough CPU/Memory? ‚Üí Node badhao
Pod ke resource requests zyada hain?
Fix:

Pod ke resource requests kam karo
Ya cluster autoscaler se naya node add hone do
Ya manually node group scale-up karo


‚úÖ 3. NodeSelector / Taints issue
Pod kisi specific node label ya taint ko match nahi kar raha hota.
Fix:

Labels theek karo
Taints ko remove/adjust karo
Pod ke tolerations add karo


‚úÖ 4. PVC / Storage issue
Agar pod ek PVC use karta hai aur wo provision nahi ho rahi:
Fix:

StorageClass verify karo
Volume size/zone match karo
EBS permission issues check karo (EKS)


‚úÖ 5. ImagePull issue
Private repo? Wrong credentials?
Fix:

imagePullSecrets add karo
Image name/tag sahi karo

------------------------------------------------------------------------

How to Fix Node NotReady ‚Äî :
Agar Kubernetes/EKS me koi node NotReady dikh raha ho, to iska simple matlab hai ki node 
healthy nahi hai ya control plane se connect nahi kar pa raha. Isko fix karne ka easy 
troubleshooting flow yeh hai

Node NotReady usually kubelet, CNI, disk, ya network issue hota hai ‚Äî main describe se 
reason check karta hoon, kubelet restart karta hoon, disk clean karta hoon, aur last 
option me node reboot/replace karta hoon.

‚úÖ 1. Node ka exact reason dekho
Sabse pehle:
Shellkubectl describe node <node-name>Show more lines
Yahan se pata chalega issue kya hai ‚Äî DiskPressure, MemoryPressure, NetworkUnavailable, kubelet crash, CNI issue, etc.

‚úÖ 2. Kubelet ko restart karo
Node par SSH karke:
Shellsudo systemctl restart kubeletShow more lines
80% cases me kubelet restart se node Ready ho jaata hai.

‚úÖ 3. CNI (aws-cni / calico) check karo
Network plugin girne se node NotReady ho jata hai.
Shellkubectl -n kube-system get pods | grep cnikubectl -n kube-system logs <aws-cni-pod>Show more lines

‚úÖ 4. Disk full? Clean karo
Disk 100% ho jaye to kubelet hang ho jata hai.
Shelldf -hdocker system prune -afsudo journalctl --vacuum-size=100MShow more lines

‚úÖ 5. Node API server se connect ho raha hai?
Security group, network route table, ENI check karo.
Agar node API server ko hit nahi kar pa raha ‚Üí NotReady.

‚úÖ 6. Container runtime restart
Shellsudo systemctl restart containerdShow more lines
Ya
Shellsudo systemctl restart dockerShow more lines

‚úÖ 7. Final fix ‚Äî reboot or replace
Shellsudo rebootShow more lines
Aur agar phir bhi issue na solve ho:
Node ko terminate kar do.
EKS managed node group automatically naya healthy node bana dega.
